# 第一周周报

## 一. 网络结构分析

论文提供的网络结构示意图：

![net](.\pic\net.PNG)

### **1. Input layer**：

输入数据是三个方向头部的角度：pan, tilt 和roll.

某一方向的角度包括了一个数据窗口（$W=250ms,\ \Delta t=0.0125s$）,也就是包括自己在内的前21个数据。为了增强网络的泛化能力，将角度变化为**归一化**后的差分值。

归一化公式如下：

![gongshi](.\pic\gongshi.PNG)

### **2. Convolution layer**:

使用kernel-size为10的卷积层, 作为loss-pass filter来降低噪声。

### **3. `GRU`与Convolution循环:**

**经过探究`GRU`的具体实现代码，我们发现，其实可以更改`GRU`网络中的`hidden_size`来更改我们的输出维度。将输入的30×1的数据变成30×2维的数据。**

**也就是说在`GRU`层后再使用二维卷积是可行的。以此循环六次，以此计算最显著的特征。**

### **4. Max Pooling:**

池化层用于离散化与下采样最后一层的卷积的输出。

### 5. `FNN`

经过一个全连接层后再重新将数值映射为头部位置的值。

### **注：**

由于作者对网络结构并没有进行详细的阐述，所以我们网络中的一些参数还是有待商榷的，包括`hidden_size`的设置，这些参数还要我们在具体的训练过程中调节。

## 二. 备选网络

我们计划在搭建上述网络的同时，同时实现2018年论文中提到的如下网络：（该网络的结构较为简单，在论文中也显示出其效果并不差，方便与我们上述搭建的网络进行比较）

![simple](.\pic\simple.PNG)

## 三. 损失函数的设置

根据论文中作者的选择，我们计划采用Mean Absolute Error(MAE), 作为loss function

<img src=".\pic\shizi.png" alt="shizi" style="zoom:50%;" />

对应的`pytorch`指令：

```python
torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor
```

## 四. 数据

我们已经联系了企业的工作人员，并拿到了部分样本数据。我们计划在第二周采集更多的数据。

企业提供的数据格式如下：

![data_explaination](.\pic\data_explaination.png)

其中我们需要的数据为四元数：x,y,z,w

与模型的输入数据的转换关系如下：

![train](D:\模式识别\头部姿态预测\第1周\pic\train.PNG)

我们在网上找到实现转换的代码：https://github.com/tuxcell/PSpincalc

我们会把相应的代码嵌入到我们预处理数据的代码中。

## 五. `GRU`的实现

经过研究`GRU`的代码，发现之前的疑问是由于忽略了一个严峻的现实，**我们可以通过修改GRU网络中的`hidden_size`来解决我们的问题，`hidden_size`是hidden state的特征数量，该参数决定了我们输出的形状**。输出的shape是`(seq_len, batch, num_directions*hidden_size)`，我们可以将一维的数据变成二维的数据。

[GRU官方文档](https://pytorch.org/docs/1.2.0/nn.html#gru)

**GRU代码：**

实现一层GRU层与一个全连接层：

```python
class GRUNet(nn.Module):
 
    def __init__(self, input_size):
        super(GRUNet, self).__init__()
        self.rnn = nn.GRU(
            input_size=input_size,
            hidden_size=64,
            num_layers=1,
            batch_first=True,
           # bidirectional=True
        )
        self.out = nn.Sequential(
            nn.Linear(128, 1)
        )
 
    def forward(self, x):
        r_out, (h_n, h_c) = self.rnn(x, None)  # None 表示 hidden state 会用全0的 state
        out = self.out(r_out[:, -1])
        print(out.shape)
        return out

```

## 六. CNN的实现

卷积层与池化层的相关代码：(搭建一个简单的CNN网络)

```python
class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        # torch.nn.MaxPool2d(kernel_size, stride, padding)
        # input 維度 [3, 128, 128]
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]

            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]

            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]

            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]
            
            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]
        )
        self.fc = nn.Sequential(
            nn.Linear(512*4*4, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 11)
        )

    def forward(self, x):
        out = self.cnn(x)
        out = out.view(out.size()[0], -1)
        return self.fc(out)
```

## 七. 总结

在本周我们完成了在计划书中提到的：

1. 精读了论文，弄清了网路结构和评估模型的方法。找到了模型的输入数据与我们的数据的异同，找到了转换公式。
2. 掌握了`GRU`与`CNN`的`pytorch`实现。
3. 与企业的相关人员沟通，获取了样本数据集。

比较遗憾的是，我们联系了论文作者，想要网络的代码，但是作者并没有回复。

## 八. 下周安排

1. 向企业学习采集数据的方法，获取大规模的数据。
2. 进行数据预处理，数据的转换，正规化等。
3. 结合本周对神经网络的研究，完成神经网络的初步搭建。